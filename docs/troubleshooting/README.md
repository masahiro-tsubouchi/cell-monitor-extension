# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

> **å¯¾è±¡**: é–‹ç™ºè€…ãƒ»é‹ç”¨æ‹…å½“è€…
> **æœ€çµ‚æ›´æ–°æ—¥**: 2025-01-18
> **ç·Šæ€¥åº¦**: é«˜ãƒ»ä¸­ãƒ»ä½ã§åˆ†é¡

## ğŸš¨ ç·Šæ€¥åº¦åˆ¥å•é¡Œåˆ†é¡

### ğŸ”´ é«˜ç·Šæ€¥åº¦ï¼ˆå³åº§ã«å¯¾å¿œãŒå¿…è¦ï¼‰
- ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®åœæ­¢
- ãƒ‡ãƒ¼ã‚¿æå¤±ã®å¯èƒ½æ€§
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ
- å¤§é‡ã®ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ

### ğŸŸ¡ ä¸­ç·Šæ€¥åº¦ï¼ˆ24æ™‚é–“ä»¥å†…ã«å¯¾å¿œï¼‰
- ä¸€éƒ¨æ©Ÿèƒ½ã®åœæ­¢
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–
- æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®é »ç™º
- ãƒ­ã‚°ã®ç•°å¸¸

### ğŸŸ¢ ä½ç·Šæ€¥åº¦ï¼ˆè¨ˆç”»çš„ã«å¯¾å¿œï¼‰
- è»½å¾®ãªæ©Ÿèƒ½ä¸å…·åˆ
- UI/UXã®æ”¹å–„
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ›´æ–°
- æœ€é©åŒ–ã®ææ¡ˆ

## ğŸ”§ ä¸€èˆ¬çš„ãªå•é¡Œã¨è§£æ±ºæ–¹æ³•

### 1. Dockerç’°å¢ƒã®å•é¡Œ

#### å•é¡Œ: ã‚³ãƒ³ãƒ†ãƒŠãŒèµ·å‹•ã—ãªã„
```bash
# ç—‡çŠ¶
docker-compose up -d
# ã‚¨ãƒ©ãƒ¼: container failed to start

# è¨ºæ–­æ‰‹é †
docker-compose logs [service_name]
docker ps -a
docker system df

# è§£æ±ºæ–¹æ³•
# 1. ãƒ­ã‚°ã®ç¢ºèª
docker-compose logs --tail=50 postgres
docker-compose logs --tail=50 redis
docker-compose logs --tail=50 influxdb

# 2. ãƒãƒ¼ãƒˆç«¶åˆã®ç¢ºèª
sudo lsof -i :5432  # PostgreSQL
sudo lsof -i :6379  # Redis
sudo lsof -i :8086  # InfluxDB

# 3. ç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
docker-compose down -v
docker system prune -f
docker-compose up -d
```

#### å•é¡Œ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼
```bash
# ç—‡çŠ¶
sqlalchemy.exc.OperationalError: could not connect to server

# è¨ºæ–­æ‰‹é †
# 1. ã‚³ãƒ³ãƒ†ãƒŠã®çŠ¶æ…‹ç¢ºèª
docker-compose ps

# 2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª
docker network ls
docker network inspect jupyter-extensionver2_default

# 3. æ¥ç¶šãƒ†ã‚¹ãƒˆ
docker-compose exec postgres psql -U cellmonitor_user -d cellmonitor_db

# è§£æ±ºæ–¹æ³•
# 1. ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
echo $DATABASE_URL
echo $POSTGRES_USER
echo $POSTGRES_PASSWORD

# 2. æ¥ç¶šæ–‡å­—åˆ—ã®ä¿®æ­£
# æ­£ã—ã„å½¢å¼: postgresql://user:password@host:port/database
DATABASE_URL=postgresql://cellmonitor_user:cellmonitor_password@postgres:5432/cellmonitor_db

# 3. ã‚³ãƒ³ãƒ†ãƒŠã®å†èµ·å‹•
docker-compose restart postgres
docker-compose restart fastapi
```

### 2. FastAPI ã‚µãƒ¼ãƒãƒ¼ã®å•é¡Œ

#### å•é¡Œ: API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå¿œç­”ã—ãªã„
```bash
# ç—‡çŠ¶
curl: (7) Failed to connect to localhost port 8000

# è¨ºæ–­æ‰‹é †
# 1. ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ç¢ºèª
docker-compose logs fastapi

# 2. ãƒãƒ¼ãƒˆç¢ºèª
docker-compose ps
netstat -tlnp | grep 8000

# 3. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://localhost:8000/health
curl http://localhost:8000/docs

# è§£æ±ºæ–¹æ³•
# 1. ã‚µãƒ¼ãƒãƒ¼ã®å†èµ·å‹•
docker-compose restart fastapi

# 2. ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®èª¿æ•´
# main.py ã§ DEBUG ãƒ¬ãƒ™ãƒ«ã«è¨­å®š
import logging
logging.basicConfig(level=logging.DEBUG)

# 3. ä¾å­˜é–¢ä¿‚ã®ç¢ºèª
pip list | grep fastapi
pip list | grep uvicorn
```

#### å•é¡Œ: Redis æ¥ç¶šã‚¨ãƒ©ãƒ¼
```python
# ç—‡çŠ¶
redis.exceptions.ConnectionError: Error 111 connecting to redis:6379

# è¨ºæ–­æ‰‹é †
# 1. Redis ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ç¢ºèª
docker-compose exec redis redis-cli ping

# 2. æ¥ç¶šè¨­å®šã®ç¢ºèª
import redis
r = redis.Redis(host='localhost', port=6379, db=0)
r.ping()

# è§£æ±ºæ–¹æ³•
# 1. Redis ã‚³ãƒ³ãƒ†ãƒŠã®å†èµ·å‹•
docker-compose restart redis

# 2. æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®è¨­å®š
import redis
pool = redis.ConnectionPool(
    host='redis',
    port=6379,
    db=0,
    max_connections=20,
    retry_on_timeout=True
)
r = redis.Redis(connection_pool=pool)

# 3. æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®èª¿æ•´
r = redis.Redis(
    host='redis',
    port=6379,
    socket_timeout=5,
    socket_connect_timeout=5
)
```

### 3. JupyterLab æ‹¡å¼µæ©Ÿèƒ½ã®å•é¡Œ

#### å•é¡Œ: æ‹¡å¼µæ©Ÿèƒ½ãŒèª­ã¿è¾¼ã¾ã‚Œãªã„
```bash
# ç—‡çŠ¶
JupyterLab ã«æ‹¡å¼µæ©Ÿèƒ½ãŒè¡¨ç¤ºã•ã‚Œãªã„

# è¨ºæ–­æ‰‹é †
# 1. æ‹¡å¼µæ©Ÿèƒ½ã®çŠ¶æ…‹ç¢ºèª
jupyter labextension list

# 2. ãƒ“ãƒ«ãƒ‰çŠ¶æ³ã®ç¢ºèª
jupyter lab build --dev-build=False --minimize=False

# 3. ãƒ­ã‚°ã®ç¢ºèª
jupyter lab --debug

# è§£æ±ºæ–¹æ³•
# 1. æ‹¡å¼µæ©Ÿèƒ½ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cd cell-monitor-extension
npm install
npm run build
jupyter labextension install .

# 2. JupyterLab ã®å†ãƒ“ãƒ«ãƒ‰
jupyter lab clean
jupyter lab build

# 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢
jupyter lab --dev-mode=False
```

#### å•é¡Œ: ã‚¤ãƒ™ãƒ³ãƒˆãŒé€ä¿¡ã•ã‚Œãªã„
```javascript
// ç—‡çŠ¶
console.log ã« "Event sent" ãŒè¡¨ç¤ºã•ã‚Œãªã„

// è¨ºæ–­æ‰‹é †
// 1. ãƒ–ãƒ©ã‚¦ã‚¶ã®é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ãƒ–ã‚’ç¢ºèª
// 2. ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ã®ç¢ºèª
// 3. FastAPI ã‚µãƒ¼ãƒãƒ¼ã®ãƒ­ã‚°ç¢ºèª

// è§£æ±ºæ–¹æ³•
// 1. CORS è¨­å®šã®ç¢ºèª
// FastAPI main.py
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8888"],  // JupyterLab ã®URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

// 2. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ URL ã®ç¢ºèª
// extension/src/index.ts
const API_BASE_URL = 'http://localhost:8000/api/v1';

// 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®è¿½åŠ 
try {
    const response = await fetch(`${API_BASE_URL}/events`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
        },
        body: JSON.stringify(eventData)
    });

    if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
    }

    console.log('Event sent successfully');
} catch (error) {
    console.error('Failed to send event:', error);
}
```

### 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ã®å•é¡Œ

#### å•é¡Œ: PostgreSQL ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–
```sql
-- ç—‡çŠ¶
-- ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œæ™‚é–“ãŒç•°å¸¸ã«é•·ã„

-- è¨ºæ–­æ‰‹é †
-- 1. å®Ÿè¡Œä¸­ã®ã‚¯ã‚¨ãƒªã®ç¢ºèª
SELECT pid, now() - pg_stat_activity.query_start AS duration, query
FROM pg_stat_activity
WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes';

-- 2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½¿ç”¨çŠ¶æ³ç¢ºèª
SELECT
    schemaname,
    tablename,
    attname,
    n_distinct,
    correlation
FROM pg_stats
WHERE tablename = 'learning_sessions';

-- 3. ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- è§£æ±ºæ–¹æ³•
-- 1. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¿½åŠ 
CREATE INDEX idx_learning_sessions_user_id ON learning_sessions(user_id);
CREATE INDEX idx_learning_sessions_timestamp ON learning_sessions(created_at);
CREATE INDEX idx_learning_sessions_notebook ON learning_sessions(notebook_path);

-- 2. çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
ANALYZE learning_sessions;
ANALYZE users;
ANALYZE notebooks;

-- 3. ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
DELETE FROM learning_sessions
WHERE created_at < NOW() - INTERVAL '6 months';

-- 4. VACUUM ã®å®Ÿè¡Œ
VACUUM ANALYZE learning_sessions;
```

#### å•é¡Œ: InfluxDB ã®æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼
```python
# ç—‡çŠ¶
influxdb_client.rest.ApiException: (400) Bad Request

# è¨ºæ–­æ‰‹é †
# 1. InfluxDB ã®çŠ¶æ…‹ç¢ºèª
docker-compose exec influxdb influx ping

# 2. ãƒã‚±ãƒƒãƒˆã®ç¢ºèª
docker-compose exec influxdb influx bucket list

# 3. æ›¸ãè¾¼ã¿æ¨©é™ã®ç¢ºèª
docker-compose exec influxdb influx auth list

# è§£æ±ºæ–¹æ³•
# 1. ãƒã‚±ãƒƒãƒˆã®ä½œæˆ
from influxdb_client import InfluxDBClient

client = InfluxDBClient(
    url="http://localhost:8086",
    token="your-token",
    org="your-org"
)

# ãƒã‚±ãƒƒãƒˆã®ä½œæˆ
buckets_api = client.buckets_api()
bucket = buckets_api.create_bucket(
    bucket_name="cellmonitor",
    org="your-org"
)

# 2. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ç¢ºèª
from influxdb_client import Point

point = Point("cell_execution") \
    .tag("user_id", "user123") \
    .tag("notebook", "lesson1") \
    .field("duration_ms", 150) \
    .field("has_error", False) \
    .time(datetime.utcnow(), WritePrecision.NS)

# 3. ãƒãƒƒãƒæ›¸ãè¾¼ã¿ã®å®Ÿè£…
write_api = client.write_api(write_options=SYNCHRONOUS)
points = []
for event in events:
    point = Point("cell_execution") \
        .tag("user_id", event["userId"]) \
        .field("duration_ms", event["executionDurationMs"])
    points.append(point)

write_api.write(bucket="cellmonitor", record=points)
```

### 5. WebSocket æ¥ç¶šã®å•é¡Œ

#### å•é¡Œ: WebSocket æ¥ç¶šãŒé »ç¹ã«åˆ‡æ–­ã•ã‚Œã‚‹
```javascript
// ç—‡çŠ¶
WebSocket connection closed unexpectedly

// è¨ºæ–­æ‰‹é †
// 1. ãƒ–ãƒ©ã‚¦ã‚¶ã®é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã§WebSocketãƒ­ã‚°ã‚’ç¢ºèª
// 2. ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ã®WebSocketãƒ­ã‚°ã‚’ç¢ºèª
// 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ³ã®ç¢ºèª

// è§£æ±ºæ–¹æ³•
// 1. å†æ¥ç¶šæ©Ÿèƒ½ã®å®Ÿè£…
class WebSocketManager {
    constructor(url) {
        this.url = url;
        this.reconnectInterval = 5000;
        this.maxReconnectAttempts = 10;
        this.reconnectAttempts = 0;
        this.connect();
    }

    connect() {
        this.ws = new WebSocket(this.url);

        this.ws.onopen = () => {
            console.log('WebSocket connected');
            this.reconnectAttempts = 0;
        };

        this.ws.onclose = () => {
            console.log('WebSocket disconnected');
            this.reconnect();
        };

        this.ws.onerror = (error) => {
            console.error('WebSocket error:', error);
        };
    }

    reconnect() {
        if (this.reconnectAttempts < this.maxReconnectAttempts) {
            this.reconnectAttempts++;
            setTimeout(() => {
                console.log(`Reconnecting... (${this.reconnectAttempts}/${this.maxReconnectAttempts})`);
                this.connect();
            }, this.reconnectInterval);
        }
    }
}

// 2. ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆæ©Ÿèƒ½ã®å®Ÿè£…
// ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ (FastAPI)
@app.websocket("/ws/dashboard")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡
            await websocket.send_json({"type": "heartbeat"})
            await asyncio.sleep(30)  # 30ç§’é–“éš”
    except WebSocketDisconnect:
        print("WebSocket disconnected")

// ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰
this.ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    if (data.type === 'heartbeat') {
        // ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã®å¿œç­”
        this.ws.send(JSON.stringify({type: 'heartbeat_response'}));
    }
};
```

## ğŸ” ãƒ‡ãƒãƒƒã‚°æ‰‹æ³•

### 1. ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®èª¿æ•´
```python
# FastAPI ã‚µãƒ¼ãƒãƒ¼ã®ãƒ­ã‚°è¨­å®š
import logging

# é–‹ç™ºç’°å¢ƒ
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æœ¬ç•ªç’°å¢ƒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# ç‰¹å®šã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)
logging.getLogger('redis').setLevel(logging.WARNING)
```

### 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
```python
# å®Ÿè¡Œæ™‚é–“ã®æ¸¬å®š
import time
import functools

def measure_time(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"{func.__name__} took {end_time - start_time:.4f} seconds")
        return result
    return wrapper

@measure_time
def process_event(event_data):
    # å‡¦ç†ã®å®Ÿè£…
    pass

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
import psutil
import os

def log_memory_usage():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    print(f"Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB")

# CPUä½¿ç”¨ç‡ã®ç›£è¦–
def log_cpu_usage():
    cpu_percent = psutil.cpu_percent(interval=1)
    print(f"CPU usage: {cpu_percent}%")
```

### 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–
```python
# SQLAlchemy ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œè¨ˆç”»ç¢ºèª
from sqlalchemy import text

# EXPLAIN ã®å®Ÿè¡Œ
result = session.execute(
    text("EXPLAIN ANALYZE SELECT * FROM learning_sessions WHERE user_id = :user_id"),
    {"user_id": "user123"}
)
for row in result:
    print(row)

# ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªã®ç‰¹å®š
import time
from sqlalchemy import event
from sqlalchemy.engine import Engine

@event.listens_for(Engine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    context._query_start_time = time.time()

@event.listens_for(Engine, "after_cursor_execute")
def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - context._query_start_time
    if total > 0.1:  # 100msä»¥ä¸Šã®ã‚¯ã‚¨ãƒªã‚’ãƒ­ã‚°å‡ºåŠ›
        print(f"Slow query: {total:.4f}s - {statement[:100]}...")
```

## ğŸ“Š ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

### 1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
```python
# FastAPI ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
@app.get("/health")
async def health_check():
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {}
    }

    # PostgreSQL ã®ç¢ºèª
    try:
        db_session.execute(text("SELECT 1"))
        health_status["services"]["postgresql"] = "healthy"
    except Exception as e:
        health_status["services"]["postgresql"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"

    # Redis ã®ç¢ºèª
    try:
        redis_client.ping()
        health_status["services"]["redis"] = "healthy"
    except Exception as e:
        health_status["services"]["redis"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"

    # InfluxDB ã®ç¢ºèª
    try:
        influx_client.ping()
        health_status["services"]["influxdb"] = "healthy"
    except Exception as e:
        health_status["services"]["influxdb"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"

    return health_status
```

### 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
```python
# Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹
from prometheus_client import Counter, Histogram, Gauge

# ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
event_counter = Counter('events_total', 'Total number of events', ['event_type'])
error_counter = Counter('errors_total', 'Total number of errors', ['error_type'])

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
request_duration = Histogram('request_duration_seconds', 'Request duration')
db_query_duration = Histogram('db_query_duration_seconds', 'Database query duration')

# ã‚²ãƒ¼ã‚¸
active_connections = Gauge('active_connections', 'Number of active connections')

# ä½¿ç”¨ä¾‹
@app.post("/api/v1/events")
async def receive_event(event_data: dict):
    with request_duration.time():
        event_counter.labels(event_type=event_data['eventType']).inc()

        try:
            # ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†
            result = process_event(event_data)
            return result
        except Exception as e:
            error_counter.labels(error_type=type(e).__name__).inc()
            raise
```

## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
```sql
-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥
CREATE INDEX CONCURRENTLY idx_learning_sessions_user_timestamp
ON learning_sessions(user_id, created_at DESC);

CREATE INDEX CONCURRENTLY idx_learning_sessions_notebook_timestamp
ON learning_sessions(notebook_path, created_at DESC);

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
CREATE TABLE learning_sessions_2025_01 PARTITION OF learning_sessions
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

-- çµ±è¨ˆæƒ…å ±ã®è‡ªå‹•æ›´æ–°
ALTER TABLE learning_sessions SET (autovacuum_analyze_scale_factor = 0.02);
```

### 2. Redis æœ€é©åŒ–
```python
# æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®è¨­å®š
import redis

pool = redis.ConnectionPool(
    host='redis',
    port=6379,
    db=0,
    max_connections=50,
    retry_on_timeout=True,
    health_check_interval=30
)

redis_client = redis.Redis(connection_pool=pool)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
pipe = redis_client.pipeline()
for event in events:
    pipe.publish('events', json.dumps(event))
pipe.execute()
```

### 3. FastAPI æœ€é©åŒ–
```python
# éåŒæœŸå‡¦ç†ã®æ´»ç”¨
import asyncio
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=10)

@app.post("/api/v1/events")
async def receive_event(event_data: dict):
    # å³åº§ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
    event_id = generate_event_id()

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å‡¦ç†
    asyncio.create_task(process_event_async(event_data, event_id))

    return {"status": "received", "event_id": event_id}

async def process_event_async(event_data: dict, event_id: str):
    # é‡ã„å‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(executor, heavy_processing, event_data)
```

---

## ğŸ“ ã‚µãƒãƒ¼ãƒˆæƒ…å ±

### é–‹ç™ºãƒãƒ¼ãƒ é€£çµ¡å…ˆ
- **ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒ†ãƒŠãƒ¼**: [é€£çµ¡å…ˆæƒ…å ±]
- **ç·Šæ€¥æ™‚é€£çµ¡**: [ç·Šæ€¥é€£çµ¡å…ˆ]
- **Issueå ±å‘Š**: [GitHubãƒªãƒã‚¸ãƒˆãƒªURL]

### é–¢é€£ãƒªã‚½ãƒ¼ã‚¹
- [ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰](http://monitoring.example.com)
- [ãƒ­ã‚°é›†ç´„ã‚·ã‚¹ãƒ†ãƒ ](http://logs.example.com)
- [API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](http://localhost:8000/docs)

---

## ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](../architecture/README.md)
- [APIä»•æ§˜æ›¸](../api/README.md)
- [ãƒ†ã‚¹ãƒˆæˆ¦ç•¥](../testing/README.md)
- [é–‹ç™ºè¨ˆç”»](../development/DEVELOPMENT_PLAN.md)
