# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°è¨­è¨ˆ

> **è¨­è¨ˆæ–¹é‡**: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹æˆã«ã‚ˆã‚‹ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ
> **æœ€çµ‚æ›´æ–°æ—¥**: 2025-01-18

## ğŸ¯ è¨­è¨ˆç›®æ¨™

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¦ä»¶
- **åŒæ™‚æ¥ç¶šæ•°**: 200+ ç”Ÿå¾’ã®åŒæ™‚å­¦ç¿’æ´»å‹•
- **ãƒ‡ãƒ¼ã‚¿å‡¦ç†é‡**: æ¯ç§’æ•°ç™¾ã®ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†
- **å¿œç­”æ™‚é–“**: APIå¿œç­” < 100ms, ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥ < 1ç§’

### ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®åˆ†æ
- **ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿**: å¤‰æ›´é »åº¦ä½ã€é–¢ä¿‚æ€§é‡è¦ã€ACIDç‰¹æ€§å¿…é ˆ
- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿**: å¤§é‡ã€é«˜é »åº¦ã€æ™‚é–“è»¸ã§ã®åˆ†æé‡è¦
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿**: ä¸€æ™‚çš„ã€é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ã€æ®ç™ºæ€§è¨±å®¹

## ğŸ—ï¸ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹æˆ

### PostgreSQLï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ï¼‰

```sql
-- ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(100) UNIQUE NOT NULL,
    display_name VARCHAR(200),
    role VARCHAR(50) DEFAULT 'student',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç®¡ç†
CREATE TABLE notebooks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    path VARCHAR(500) UNIQUE NOT NULL,
    title VARCHAR(200),
    description TEXT,
    cell_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
CREATE TABLE learning_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    notebook_id UUID REFERENCES notebooks(id),
    started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    ended_at TIMESTAMP WITH TIME ZONE,
    total_duration_ms BIGINT,
    cell_executions INTEGER DEFAULT 0,
    error_count INTEGER DEFAULT 0
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_notebooks_path ON notebooks(path);
CREATE INDEX idx_sessions_user_notebook ON learning_sessions(user_id, notebook_id);
CREATE INDEX idx_sessions_started_at ON learning_sessions(started_at);
```

### InfluxDBï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢ï¼‰

```sql
-- ã‚»ãƒ«å®Ÿè¡Œã‚¤ãƒ™ãƒ³ãƒˆ
-- Measurement: cell_executions
-- Tags: user_id, notebook_path, cell_index, event_type
-- Fields: execution_duration_ms, has_error, error_message, execution_count
-- Time: event_timestamp

-- å­¦ç¿’é€²æ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹
-- Measurement: learning_progress
-- Tags: user_id, notebook_path
-- Fields: completed_cells, total_cells, progress_percentage, session_duration_ms
-- Time: progress_timestamp

-- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
-- Measurement: performance_metrics
-- Tags: user_id, notebook_path, cell_type
-- Fields: avg_execution_time, max_execution_time, error_rate
-- Time: metric_timestamp
```

### Redisï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰

```redis
# Pub/Sub ãƒãƒ£ãƒãƒ«è¨­è¨ˆ
PUBLISH progress_events '{"event_type": "cell_execution", "user_id": "...", "data": {...}}'
PUBLISH error_events '{"event_type": "execution_error", "user_id": "...", "data": {...}}'
PUBLISH session_events '{"event_type": "session_start", "user_id": "...", "data": {...}}'

# WebSocketæ¥ç¶šç®¡ç†
HSET websocket_connections connection_id user_id
HSET websocket_connections connection_id last_activity

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥
SETEX session:user_123 3600 '{"current_notebook": "...", "last_activity": "..."}'

# é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
SETEX class_summary:notebook_abc 300 '{"total_students": 25, "avg_progress": 0.75}'
```

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¨­è¨ˆ

### 1. ã‚¤ãƒ™ãƒ³ãƒˆå—ä¿¡ãƒ»æŒ¯ã‚Šåˆ†ã‘

```python
# FastAPI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/api/v1/events")
async def receive_event(event: EventData):
    # 1. å³åº§ã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å¿œç­”
    await redis_client.publish("progress_events", event.json())
    return {"status": "received", "event_id": event.id}

# éåŒæœŸãƒ¯ãƒ¼ã‚«ãƒ¼
async def process_events():
    async for message in redis_client.subscribe("progress_events"):
        event = EventData.parse_raw(message)
        await route_event(event)

async def route_event(event: EventData):
    if event.event_type == "cell_execution":
        await store_cell_execution(event)
    elif event.event_type == "session_start":
        await store_session_data(event)
    # ... ãã®ä»–ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—
```

### 2. ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–æˆ¦ç•¥

```python
async def store_cell_execution(event: CellExecutionEvent):
    # PostgreSQL: ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±æ›´æ–°
    await update_session_stats(event.user_id, event.notebook_path)

    # InfluxDB: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    point = Point("cell_executions") \
        .tag("user_id", event.user_id) \
        .tag("notebook_path", event.notebook_path) \
        .tag("cell_index", event.cell_index) \
        .field("execution_duration_ms", event.duration) \
        .field("has_error", event.has_error) \
        .time(event.timestamp)

    await influx_client.write_api().write(point=point)

    # Redis: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥
    await notify_websocket_clients(event)
```

## ğŸ“Š ã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–

### é«˜é »åº¦ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–

```sql
-- 1. ã‚¯ãƒ©ã‚¹å…¨ä½“ã®é€²æ—ã‚µãƒãƒªãƒ¼ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ï¼‰
SELECT
    u.username,
    ls.notebook_id,
    ls.cell_executions,
    ls.error_count,
    ls.total_duration_ms
FROM learning_sessions ls
JOIN users u ON ls.user_id = u.id
WHERE ls.started_at >= NOW() - INTERVAL '1 day'
ORDER BY ls.started_at DESC;

-- 2. å€‹åˆ¥ç”Ÿå¾’ã®è©³ç´°é€²æ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ´»ç”¨ï¼‰
SELECT * FROM learning_sessions
WHERE user_id = $1 AND notebook_id = $2
ORDER BY started_at DESC LIMIT 10;
```

### InfluxDBé›†è¨ˆã‚¯ã‚¨ãƒª

```sql
-- æ™‚é–“åˆ¥ã®å­¦ç¿’æ´»å‹•é‡
SELECT mean("execution_duration_ms") as avg_duration,
       count("execution_duration_ms") as execution_count
FROM "cell_executions"
WHERE time >= now() - 1h
GROUP BY time(10m), "user_id"

-- ã‚¨ãƒ©ãƒ¼ç‡ã®æ¨ç§»
SELECT count("has_error") as error_count,
       count(*) as total_executions
FROM "cell_executions"
WHERE time >= now() - 24h
GROUP BY time(1h), "notebook_path"
```

## ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–

```sql
-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆPostgreSQLï¼‰
CREATE TABLE learning_sessions_2025_01 PARTITION OF learning_sessions
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
CREATE INDEX CONCURRENTLY idx_sessions_composite
ON learning_sessions (user_id, started_at DESC, notebook_id);

-- çµ±è¨ˆæƒ…å ±æ›´æ–°
ANALYZE learning_sessions;
```

### Redisæœ€é©åŒ–

```redis
# æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
redis_pool = aioredis.ConnectionPool.from_url(
    "redis://localhost:6379",
    max_connections=20,
    retry_on_timeout=True
)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
pipe = redis_client.pipeline()
pipe.hset("session:123", "last_activity", timestamp)
pipe.expire("session:123", 3600)
await pipe.execute()
```

## ğŸ”§ é‹ç”¨ãƒ»ç›£è¦–

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

```python
async def check_database_health():
    checks = {
        "postgresql": await check_postgres_connection(),
        "influxdb": await check_influx_connection(),
        "redis": await check_redis_connection()
    }
    return checks

async def check_postgres_connection():
    try:
        result = await db.execute("SELECT 1")
        return {"status": "healthy", "response_time": "..."}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

```python
# Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹
database_query_duration = Histogram(
    'database_query_duration_seconds',
    'Database query duration',
    ['database', 'query_type']
)

database_connection_pool = Gauge(
    'database_connection_pool_size',
    'Database connection pool size',
    ['database']
)
```

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®

### ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–

```python
# æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–
from cryptography.fernet import Fernet

class EncryptedField:
    def __init__(self, key: bytes):
        self.cipher = Fernet(key)

    def encrypt(self, data: str) -> str:
        return self.cipher.encrypt(data.encode()).decode()

    def decrypt(self, encrypted_data: str) -> str:
        return self.cipher.decrypt(encrypted_data.encode()).decode()
```

### ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡

```sql
-- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ«è¨­è¨ˆ
CREATE ROLE app_reader;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO app_reader;

CREATE ROLE app_writer;
GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA public TO app_writer;

-- è¡Œãƒ¬ãƒ™ãƒ«ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
ALTER TABLE learning_sessions ENABLE ROW LEVEL SECURITY;
CREATE POLICY user_data_policy ON learning_sessions
    FOR ALL TO app_user
    USING (user_id = current_setting('app.current_user_id')::uuid);
```

## ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

### èª­ã¿å–ã‚Šå°‚ç”¨ãƒ¬ãƒ—ãƒªã‚«

```yaml
# docker-compose.yml
services:
  postgres-primary:
    image: postgres:15
    environment:
      POSTGRES_REPLICATION_MODE: master

  postgres-replica:
    image: postgres:15
    environment:
      POSTGRES_REPLICATION_MODE: slave
      POSTGRES_MASTER_SERVICE: postgres-primary
```

### ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥

```python
# ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
def get_shard_key(user_id: str) -> str:
    hash_value = hashlib.md5(user_id.encode()).hexdigest()
    shard_num = int(hash_value[:8], 16) % SHARD_COUNT
    return f"shard_{shard_num}"

async def get_user_database(user_id: str):
    shard_key = get_shard_key(user_id)
    return database_connections[shard_key]
```

---

## ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦](./README.md)
- [ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¯”è¼ƒåˆ†æ](./DATABASE_COMPARISON.md)
- [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰](./PERFORMANCE.md)
- [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­è¨ˆ](./SECURITY.md)
