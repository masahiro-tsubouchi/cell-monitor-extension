#!/usr/bin/env python3
"""
InfluxDBãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨ä¾‹:
  # éå»30æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
  python scripts/export_influxdb_metrics.py --range 30d
  
  # å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
  python scripts/export_influxdb_metrics.py --range all
  
  # ç‰¹å®šæœŸé–“ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
  python scripts/export_influxdb_metrics.py --start 2025-01-01 --end 2025-02-01
  
  # ç‰¹å®šå­¦ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
  python scripts/export_influxdb_metrics.py --student student@example.com --range 7d
  
  # ç‰¹å®šmeasurementã®ã¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
  python scripts/export_influxdb_metrics.py --range 30d --measurement student_progress
  
  # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæŒ‡å®š
  python scripts/export_influxdb_metrics.py --range 30d --output /path/to/exports
"""

import argparse
import logging
import sys
import csv
import os
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
from pathlib import Path

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.config import settings

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from influxdb_client import InfluxDBClient
    from influxdb_client.client.query_api import QueryApi
    INFLUXDB_AVAILABLE = True
except ImportError:
    logger.warning("InfluxDB client not available. Install with: pip install influxdb-client")
    INFLUXDB_AVAILABLE = False


class InfluxDBMetricsExporter:
    """InfluxDBãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "./exports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.influx_client: Optional[InfluxDBClient] = None
        self.query_api: Optional[QueryApi] = None
        
        if not INFLUXDB_AVAILABLE:
            raise RuntimeError("InfluxDB client is not available")
        
        self._connect_to_influxdb()
    
    def _connect_to_influxdb(self):
        """InfluxDBã«æ¥ç¶š"""
        try:
            self.influx_client = InfluxDBClient(
                url=settings.DYNAMIC_INFLUXDB_URL,
                token=settings.INFLUXDB_TOKEN,
                org=settings.INFLUXDB_ORG
            )
            
            self.query_api = self.influx_client.query_api()
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            health = self.influx_client.health()
            if health.status == "pass":
                logger.info("âœ… InfluxDBæ¥ç¶šæˆåŠŸ")
            else:
                raise RuntimeError(f"InfluxDBæ¥ç¶šå¤±æ•—: {health.message}")
                
        except Exception as e:
            logger.error(f"âŒ InfluxDBæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def export_metrics(
        self,
        time_range: str = "30d",
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        student_email: Optional[str] = None,
        measurement_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        
        # æ™‚é–“ç¯„å›²ã®è¨­å®š
        if time_range == "all":
            start_time = "1970-01-01T00:00:00Z"
            end_time = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
            logger.info("ğŸ“… å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™")
        elif time_range.endswith('d'):
            days = int(time_range[:-1])
            start_time = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%dT%H:%M:%SZ")
            end_time = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ") 
            logger.info(f"ğŸ“… éå»{days}æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™")
        elif start_date and end_date:
            start_time = datetime.fromisoformat(start_date).strftime("%Y-%m-%dT%H:%M:%SZ")
            end_time = datetime.fromisoformat(end_date).strftime("%Y-%m-%dT%H:%M:%SZ")
            logger.info(f"ğŸ“… {start_date} ã‹ã‚‰ {end_date} ã®æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™")
        else:
            raise ValueError("--range ã¾ãŸã¯ --start/--end ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        
        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡measurement
        measurements = [
            "student_progress",
            "cell_execution_metrics",
            "performance_data", 
            "help_request_events"
        ]
        
        if measurement_filter:
            measurements = [m for m in measurements if measurement_filter in m]
            logger.info(f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: '{measurement_filter}' ã‚’å«ã‚€measurementã®ã¿")
        
        exported_files = []
        total_records = 0
        
        for measurement in measurements:
            try:
                # ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
                file_info = self._export_measurement_data(
                    measurement=measurement,
                    start_time=start_time,
                    end_time=end_time,
                    student_email=student_email
                )
                
                if file_info:
                    exported_files.append(file_info)
                    total_records += file_info['record_count']
                    
            except Exception as e:
                logger.error(f"âŒ {measurement} ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚µãƒãƒªãƒ¼
        result = {
            "export_completed": True,
            "time_range": {
                "start": start_time,
                "end": end_time
            },
            "student_filter": student_email,
            "measurement_filter": measurement_filter,
            "exported_files": exported_files,
            "total_records": total_records,
            "output_directory": str(self.output_dir),
            "exported_at": datetime.utcnow().isoformat()
        }
        
        logger.info(f"âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {total_records:,} ãƒ¬ã‚³ãƒ¼ãƒ‰ã€{len(exported_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        
        return result
    
    def _export_measurement_data(
        self,
        measurement: str,
        start_time: str,
        end_time: str,
        student_email: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """æŒ‡å®šmeasurement ã®ãƒ‡ãƒ¼ã‚¿ã‚’CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        
        # Fluxã‚¯ã‚¨ãƒªæ§‹ç¯‰
        query = f'''
        from(bucket: "{settings.INFLUXDB_BUCKET}")
          |> range(start: {start_time}, stop: {end_time})
          |> filter(fn: (r) => r["_measurement"] == "{measurement}")
        '''
        
        if student_email:
            query += f'  |> filter(fn: (r) => r["emailAddress"] == "{student_email}")\n'
        
        query += '''
          |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
          |> sort(columns: ["_time"])
        '''
        
        try:
            # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            logger.info(f"ğŸ” {measurement} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            result = self.query_api.query(query)
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            records = []
            for table in result:
                for record in table.records:
                    record_data = {
                        "time": record.get_time().isoformat() if record.get_time() else None,
                        "measurement": record.get_measurement(),
                    }
                    
                    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    for key, value in record.values.items():
                        if not key.startswith('_') or key in ['_value', '_field']:
                            record_data[key] = value
                    
                    records.append(record_data)
            
            if not records:
                logger.info(f"â„¹ï¸ {measurement}: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            filename_parts = [f"influxdb_{measurement}"]
            if student_email:
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®@ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›
                safe_email = student_email.replace("@", "_").replace(".", "_")
                filename_parts.append(f"student_{safe_email}")
            filename_parts.append(self.timestamp)
            
            csv_filename = "_".join(filename_parts) + ".csv"
            csv_path = self.output_dir / csv_filename
            
            # CSVæ›¸ãè¾¼ã¿
            if records:
                # å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’åé›†
                all_fieldnames = set()
                for record in records:
                    all_fieldnames.update(record.keys())
                
                fieldnames = sorted(list(all_fieldnames))
                
                with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(records)
            
            file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)
            logger.info(f"âœ… {measurement}: {len(records):,} ãƒ¬ã‚³ãƒ¼ãƒ‰ â†’ {csv_path} ({file_size_mb:.1f}MB)")
            
            return {
                "measurement": measurement,
                "filename": csv_filename,
                "file_path": str(csv_path),
                "record_count": len(records),
                "file_size_mb": round(file_size_mb, 1)
            }
            
        except Exception as e:
            logger.error(f"âŒ {measurement} ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def export_aggregated_summary(
        self,
        time_range: str = "30d"
    ) -> Optional[str]:
        """é›†ç´„ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        
        if time_range.endswith('d'):
            days = int(time_range[:-1])
            start_time = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%dT%H:%M:%SZ")
        else:
            start_time = "-30d"
        
        # é›†ç´„ã‚¯ã‚¨ãƒª
        summary_query = f'''
        from(bucket: "{settings.INFLUXDB_BUCKET}")
          |> range(start: {start_time})
          |> group(columns: ["_measurement", "emailAddress"])
          |> aggregateWindow(every: 1d, fn: count, createEmpty: false)
          |> yield(name: "daily_counts")
        '''
        
        try:
            logger.info("ğŸ“Š é›†ç´„ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            result = self.query_api.query(summary_query)
            
            records = []
            for table in result:
                for record in table.records:
                    records.append({
                        "date": record.get_time().strftime("%Y-%m-%d") if record.get_time() else None,
                        "measurement": record.get_measurement(),
                        "student_email": record.values.get("emailAddress", ""),
                        "daily_count": record.get_value()
                    })
            
            if records:
                csv_filename = f"influxdb_daily_summary_{self.timestamp}.csv"
                csv_path = self.output_dir / csv_filename
                
                fieldnames = ["date", "measurement", "student_email", "daily_count"]
                with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(records)
                
                file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)
                logger.info(f"âœ… é›†ç´„ã‚µãƒãƒªãƒ¼: {len(records):,} ãƒ¬ã‚³ãƒ¼ãƒ‰ â†’ {csv_path} ({file_size_mb:.1f}MB)")
                
                return str(csv_path)
            else:
                logger.info("â„¹ï¸ é›†ç´„ã‚µãƒãƒªãƒ¼: ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
                
        except Exception as e:
            logger.error(f"âŒ é›†ç´„ã‚µãƒãƒªãƒ¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_export_stats(self) -> Dict[str, Any]:
        """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’å–å¾—"""
        try:
            stats_query = f'''
            from(bucket: "{settings.INFLUXDB_BUCKET}")
              |> range(start: -30d)
              |> group(columns: ["_measurement"])
              |> count()
              |> yield(name: "measurement_counts")
            '''
            
            result = self.query_api.query(stats_query)
            
            measurement_stats = {}
            total_points = 0
            
            for table in result:
                for record in table.records:
                    measurement = record.values.get("_measurement")
                    count = record.get_value()
                    if measurement and count:
                        measurement_stats[measurement] = count
                        total_points += count
            
            # æœ€å¤ãƒ»æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            time_range_query = f'''
            from(bucket: "{settings.INFLUXDB_BUCKET}")
              |> range(start: -3650d)  // 10å¹´å‰ã‹ã‚‰æ¤œç´¢
              |> first()
              |> keep(columns: ["_time"])
            '''
            
            oldest_time = None
            try:
                time_result = self.query_api.query(time_range_query)
                for table in time_result:
                    for record in table.records:
                        if record.get_time():
                            oldest_time = record.get_time().isoformat()
                            break
            except Exception:
                pass
            
            return {
                "bucket": settings.INFLUXDB_BUCKET,
                "measurements": measurement_stats,
                "total_points_30d": total_points,
                "oldest_data": oldest_time,
                "latest_check": datetime.utcnow().isoformat(),
                "exportable": total_points > 0
            }
            
        except Exception as e:
            logger.error(f"çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def close(self):
        """æ¥ç¶šã‚’é–‰ã˜ã‚‹"""
        if self.influx_client:
            self.influx_client.close()
            logger.info("InfluxDBæ¥ç¶šã‚’é–‰ã˜ã¾ã—ãŸ")


def parse_time_range(time_range: str) -> tuple:
    """æ™‚é–“ç¯„å›²æ–‡å­—åˆ—ã‚’è§£æ"""
    if time_range == "all":
        return "1970-01-01T00:00:00Z", datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    elif time_range.endswith('d'):
        days = int(time_range[:-1])
        start = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%dT%H:%M:%SZ")
        end = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        return start, end
    elif time_range.endswith('h'):
        hours = int(time_range[:-1])
        start = (datetime.utcnow() - timedelta(hours=hours)).strftime("%Y-%m-%dT%H:%M:%SZ")
        end = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        return start, end
    else:
        raise ValueError(f"ç„¡åŠ¹ãªæ™‚é–“ç¯„å›²å½¢å¼: {time_range}. ä¾‹: 30d, 24h, all")


def main():
    parser = argparse.ArgumentParser(
        description="InfluxDBãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    # æ™‚é–“ç¯„å›²æŒ‡å®šï¼ˆæ–¹æ³•1: ç¯„å›²æŒ‡å®šï¼‰
    parser.add_argument(
        "--range",
        type=str,
        help="ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç¯„å›²ï¼ˆä¾‹: 30d, 7d, 24h, allï¼‰"
    )
    
    # æ™‚é–“ç¯„å›²æŒ‡å®šï¼ˆæ–¹æ³•2: æœŸé–“æŒ‡å®šï¼‰
    parser.add_argument(
        "--start",
        type=str,
        help="é–‹å§‹æ—¥æ™‚ï¼ˆISOå½¢å¼: 2025-01-01T00:00:00ï¼‰"
    )
    parser.add_argument(
        "--end",
        type=str,
        help="çµ‚äº†æ—¥æ™‚ï¼ˆISOå½¢å¼: 2025-02-01T00:00:00ï¼‰"
    )
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    parser.add_argument(
        "--student",
        type=str,
        help="ç‰¹å®šå­¦ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŒ‡å®šï¼‰"
    )
    parser.add_argument(
        "--measurement",
        type=str,
        help="ç‰¹å®šmeasurementã®ã¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆéƒ¨åˆ†ãƒãƒƒãƒï¼‰"
    )
    
    # å‡ºåŠ›è¨­å®š
    parser.add_argument(
        "--output",
        type=str,
        default="./exports",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./exportsï¼‰"
    )
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--stats",
        action="store_true",
        help="ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã®ã¿è¡¨ç¤º"
    )
    parser.add_argument(
        "--summary",
        action="store_true",
        help="æ—¥æ¬¡é›†ç´„ã‚µãƒãƒªãƒ¼ã‚‚ç”Ÿæˆ"
    )
    
    args = parser.parse_args()
    
    # å¼•æ•°æ¤œè¨¼
    if not args.stats and not args.range and not (args.start and args.end):
        parser.error("--range ã¾ãŸã¯ --start/--end ã¾ãŸã¯ --stats ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    if args.start and not args.end:
        parser.error("--start ã‚’æŒ‡å®šã—ãŸå ´åˆã€--end ã‚‚å¿…è¦ã§ã™")
    
    try:
        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        exporter = InfluxDBMetricsExporter(output_dir=args.output)
        
        if args.stats:
            # çµ±è¨ˆæƒ…å ±ã®ã¿è¡¨ç¤º
            stats = exporter.get_export_stats()
            print("\nğŸ“Š InfluxDB ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
            print("=" * 60)
            print(f"ãƒã‚±ãƒƒãƒˆ: {stats.get('bucket', 'N/A')}")
            print(f"ç·ãƒã‚¤ãƒ³ãƒˆæ•°ï¼ˆéå»30æ—¥ï¼‰: {stats.get('total_points_30d', 0):,}")
            print(f"æœ€å¤ãƒ‡ãƒ¼ã‚¿: {stats.get('oldest_data', 'ä¸æ˜')}")
            
            print("\nğŸ“‹ Measurementåˆ¥ãƒã‚¤ãƒ³ãƒˆæ•°ï¼ˆéå»30æ—¥ï¼‰:")
            for measurement, count in stats.get('measurements', {}).items():
                print(f"  â€¢ {measurement}: {count:,} ãƒã‚¤ãƒ³ãƒˆ")
            
            if stats.get('exportable', False):
                print("\nâœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã™")
            else:
                print("\nâš ï¸ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯èƒ½ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        else:
            # ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
            result = exporter.export_metrics(
                time_range=args.range,
                start_date=args.start,
                end_date=args.end,
                student_email=args.student,
                measurement_filter=args.measurement
            )
            
            # é›†ç´„ã‚µãƒãƒªãƒ¼ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            summary_file = None
            if args.summary:
                summary_file = exporter.export_aggregated_summary(
                    time_range=args.range or "30d"
                )
            
            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            print(f"\nâœ… InfluxDB ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†")
            print("=" * 60)
            print(f"æœŸé–“: {result['time_range']['start']} ï½ {result['time_range']['end']}")
            if result['student_filter']:
                print(f"å­¦ç”Ÿãƒ•ã‚£ãƒ«ã‚¿: {result['student_filter']}")
            if result['measurement_filter']:
                print(f"Measurementãƒ•ã‚£ãƒ«ã‚¿: {result['measurement_filter']}")
            print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {result['total_records']:,}")
            print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {result['output_directory']}")
            
            print(f"\nğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ« ({len(result['exported_files'])} ãƒ•ã‚¡ã‚¤ãƒ«):")
            for file_info in result['exported_files']:
                print(f"  â€¢ {file_info['filename']}")
                print(f"    â””â”€ {file_info['record_count']:,} ãƒ¬ã‚³ãƒ¼ãƒ‰ ({file_info['file_size_mb']}MB)")
            
            if summary_file:
                print(f"\nğŸ“Š é›†ç´„ã‚µãƒãƒªãƒ¼: {os.path.basename(summary_file)}")
        
        exporter.close()
        
    except Exception as e:
        logger.error(f"âŒ ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()